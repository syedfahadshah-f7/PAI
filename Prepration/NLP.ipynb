{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: ['Hy', '!', 'My', 'name', 'is', 'Fahad', '.', 'I', 'am', 'an', 'AI', 'data', 'scientist', ',', 'currentl', 'I', 'am', 'working', 'with', 'nasa', 'and', 'I', 'do', 'grinding', 'alot', '.']\n",
      "Sentence: ['Hy!', 'My name is Fahad.', 'I am an AI data scientist, currentl I am working with nasa and I do grinding alot.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "import nltk\n",
    "\n",
    "# nltk.download('punkt_tab')\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "para = \"Hy! My name is Fahad. I am an AI data scientist, currentl I am working with nasa and I do grinding alot.\"\n",
    "\n",
    "words = word_tokenize(para)\n",
    "sent = sent_tokenize(para)\n",
    "\n",
    "print(f\"Words: {words}\")\n",
    "print(f\"Sentence: {sent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Words: ['Hy', '!', 'My', 'name', 'is', 'Fahad', '.', 'I', 'am', 'an', 'AI', 'data', 'scientist', ',', 'currentl', 'I', 'am', 'working', 'with', 'nasa', 'and', 'I', 'do', 'grinding', 'alot', '.']\n",
      "Stemming  Words: ['hy', '!', 'my', 'name', 'is', 'fahad', '.', 'i', 'am', 'an', 'ai', 'data', 'scientist', ',', 'currentl', 'i', 'am', 'work', 'with', 'nasa', 'and', 'i', 'do', 'grind', 'alot', '.']\n"
     ]
    }
   ],
   "source": [
    "#Stemming\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stemmer_words = [stemmer.stem(word) for word in words]\n",
    "print(f\"Original Words: {words}\")\n",
    "print(f\"Stemming  Words: {stemmer_words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\fahad\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Words: ['Hy', '!', 'My', 'name', 'is', 'Fahad', '.', 'I', 'am', 'an', 'AI', 'data', 'scientist', ',', 'currentl', 'I', 'am', 'working', 'with', 'nasa', 'and', 'I', 'do', 'grinding', 'alot', '.']\n",
      "Lemmatize  Words: ['hy', '!', 'my', 'name', 'is', 'fahad', '.', 'i', 'am', 'an', 'ai', 'data', 'scientist', ',', 'currentl', 'i', 'am', 'work', 'with', 'nasa', 'and', 'i', 'do', 'grind', 'alot', '.']\n"
     ]
    }
   ],
   "source": [
    "#lematization\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatize_word = [lemmatizer.lemmatize(word) for word in words]\n",
    "print(f\"Original Words: {words}\")\n",
    "print(f\"Lemmatize  Words: {stemmer_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence Array\n",
      " ['The beauty of curiosity lies in its boundless nature, sparking a desire to explore the unknown and uncover hidden truths.', 'It’s the force that drives innovation, propels learning, and bridges gaps between imagination and reality.', 'From the vastness of the cosmos to the intricate design of a leaf, curiosity invites us to question, understand, and marvel.', 'It’s not just about finding answers but about embracing the journey of discovery, where every step uncovers new possibilities and perspectives.', 'In a world brimming with mysteries, curiosity keeps our spirit alive, reminding us that there’s always more to learn and experience.']\n",
      "After removing Stopwords and apply lemmatization\n",
      " ['The beauty curiosity lie boundless nature , sparking desire explore unknown uncover hidden truth .', 'It ’ force drive innovation , propels learning , bridge gap imagination reality .', 'From vastness cosmos intricate design leaf , curiosity invite u question , understand , marvel .', 'It ’ finding answer embracing journey discovery , every step uncovers new possibility perspective .', 'In world brimming mystery , curiosity keep spirit alive , reminding u ’ always learn experience .'] \n"
     ]
    }
   ],
   "source": [
    "#para Lemmatize also excluding stopwords\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords,wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# nltk.download(\"stopwords\")\n",
    "# nltk.download(\"wordnet\")\n",
    "\n",
    "para = \"The beauty of curiosity lies in its boundless nature, sparking a desire to explore the unknown and uncover hidden truths. It’s the force that drives innovation, propels learning, and bridges gaps between imagination and reality. From the vastness of the cosmos to the intricate design of a leaf, curiosity invites us to question, understand, and marvel. It’s not just about finding answers but about embracing the journey of discovery, where every step uncovers new possibilities and perspectives. In a world brimming with mysteries, curiosity keeps our spirit alive, reminding us that there’s always more to learn and experience.\"\n",
    "\n",
    "sentences = sent_tokenize(para)\n",
    "Stopwords = set(stopwords.words(\"english\"))\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "print(f\"Original Sentence Array\\n {sentences}\")    \n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words = word_tokenize(sentences[i])\n",
    "    lem_words = [lem.lemmatize(word) for word in words if word not in Stopwords]\n",
    "    sentences[i] = \" \".join(lem_words)\n",
    "    \n",
    "print(f\"After removing Stopwords and apply lemmatization\\n {sentences} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words after removing puntuation: ['Hy', 'My', 'name', 'is', 'Fahad', 'I', 'am', 'an', 'AI', 'data', 'scientist', 'currentl', 'I', 'am', 'working', 'with', 'nasa', 'and', 'I', 'do', 'grinding', 'alot']\n",
      "After Removing Puntuation: Hy My name is Fahad I am an AI data scientist currentl I am working with nasa and I do grinding alot\n"
     ]
    }
   ],
   "source": [
    "#reomving puntuation \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "puntuation = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "para = \"Hy! My name is Fahad. I am an AI data scientist, currentl I am working with nasa and I do grinding alot.\"\n",
    "\n",
    "words = puntuation.tokenize(para)\n",
    "\n",
    "para = \" \".join(words)\n",
    "print(f\"Words after removing puntuation: {words}\")\n",
    "print(f\"After Removing Puntuation: {para}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectorizer\n",
      " [[0 0 0 1 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      "  1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0\n",
      "  0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0\n",
      "  0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0\n",
      "  0 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0]\n",
      " [1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1\n",
      "  0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1]]\n",
      "TfIdf Vectorizer\n",
      " [[0.         0.         0.         0.28342702 0.28342702 0.\n",
      "  0.         0.         0.18981438 0.         0.28342702 0.\n",
      "  0.         0.         0.         0.         0.28342702 0.\n",
      "  0.         0.         0.         0.28342702 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.28342702 0.         0.\n",
      "  0.28342702 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.28342702 0.         0.         0.28342702\n",
      "  0.28342702 0.28342702 0.         0.         0.28342702 0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.32189611\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.32189611 0.         0.         0.         0.         0.\n",
      "  0.32189611 0.         0.32189611 0.         0.32189611 0.\n",
      "  0.32189611 0.         0.         0.25970376 0.         0.\n",
      "  0.         0.         0.32189611 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.32189611 0.\n",
      "  0.32189611 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.30936611 0.20718609 0.30936611 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.30936611 0.         0.         0.         0.\n",
      "  0.         0.30936611 0.30936611 0.         0.         0.\n",
      "  0.30936611 0.         0.         0.         0.30936611 0.\n",
      "  0.         0.         0.         0.         0.         0.30936611\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.30936611 0.         0.30936611\n",
      "  0.        ]\n",
      " [0.         0.         0.29296785 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.29296785\n",
      "  0.         0.29296785 0.29296785 0.         0.         0.29296785\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.23636462 0.29296785 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.29296785 0.29296785 0.29296785 0.         0.\n",
      "  0.         0.         0.         0.         0.29296785 0.\n",
      "  0.         0.         0.29296785 0.         0.         0.\n",
      "  0.        ]\n",
      " [0.29554625 0.29554625 0.         0.         0.         0.\n",
      "  0.29554625 0.         0.19793076 0.         0.         0.\n",
      "  0.         0.         0.         0.29554625 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.29554625\n",
      "  0.         0.         0.         0.         0.         0.29554625\n",
      "  0.         0.29554625 0.         0.         0.         0.29554625\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.29554625 0.         0.29554625 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.29554625]]\n"
     ]
    }
   ],
   "source": [
    "#Count vectorizer/TF IDF\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords,wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "cv = CountVectorizer(max_features= 1500)\n",
    "tf = TfidfVectorizer()\n",
    "\n",
    "# nltk.download(\"stopwords\")\n",
    "# nltk.download(\"wordnet\")\n",
    "\n",
    "para = \"The beauty of curiosity lies in its boundless nature, sparking a desire to explore the unknown and uncover hidden truths. It’s the force that drives innovation, propels learning, and bridges gaps between imagination and reality. From the vastness of the cosmos to the intricate design of a leaf, curiosity invites us to question, understand, and marvel. It’s not just about finding answers but about embracing the journey of discovery, where every step uncovers new possibilities and perspectives. In a world brimming with mysteries, curiosity keeps our spirit alive, reminding us that there’s always more to learn and experience.\"\n",
    "\n",
    "sentences = sent_tokenize(para)\n",
    "Stopwords = set(stopwords.words(\"english\"))\n",
    "lem = WordNetLemmatizer()\n",
    "corpus = [] \n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words = word_tokenize(sentences[i])\n",
    "    lem_words = [lem.lemmatize(word) for word in words if word not in Stopwords]\n",
    "    sentences[i] = \" \".join(lem_words)\n",
    "    corpus.append(sentences[i])\n",
    "    \n",
    "x = cv.fit_transform(corpus).toarray()\n",
    "print(f\"Count Vectorizer\\n {x}\")\n",
    "y =tf.fit_transform(corpus).toarray()\n",
    "print(f\"TfIdf Vectorizer\\n {y}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
